{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved for 100 timesteps\n",
      "Model saved for 200 timesteps\n",
      "Model saved for 300 timesteps\n",
      "Model saved for 400 timesteps\n",
      "Model saved for 500 timesteps\n",
      "Model saved for 600 timesteps\n",
      "Model saved for 700 timesteps\n",
      "Model saved for 800 timesteps\n",
      "Model saved for 900 timesteps\n",
      "Model saved for 1000 timesteps\n",
      "Model saved for 1100 timesteps\n",
      "Model saved for 1200 timesteps\n",
      "Model saved for 1300 timesteps\n",
      "Model saved for 1400 timesteps\n",
      "Model saved for 1500 timesteps\n",
      "Model saved for 1600 timesteps\n",
      "Model saved for 1700 timesteps\n",
      "Model saved for 1800 timesteps\n",
      "Model saved for 1900 timesteps\n",
      "Model saved for 2000 timesteps\n",
      "Model saved for 2100 timesteps\n",
      "Model saved for 2200 timesteps\n",
      "Model saved for 2300 timesteps\n",
      "Model saved for 2400 timesteps\n",
      "Model saved for 2500 timesteps\n",
      "Model saved for 2600 timesteps\n",
      "Model saved for 2700 timesteps\n",
      "Model saved for 2800 timesteps\n",
      "Model saved for 2900 timesteps\n",
      "Model saved for 3000 timesteps\n",
      "Model saved for 3100 timesteps\n",
      "Model saved for 3200 timesteps\n",
      "Model saved for 3300 timesteps\n",
      "Model saved for 3400 timesteps\n",
      "Model saved for 3500 timesteps\n",
      "Model saved for 3600 timesteps\n",
      "Model saved for 3700 timesteps\n",
      "Model saved for 3800 timesteps\n",
      "Model saved for 3900 timesteps\n",
      "Model saved for 4000 timesteps\n",
      "Model saved for 4100 timesteps\n",
      "Model saved for 4200 timesteps\n",
      "Model saved for 4300 timesteps\n",
      "Model saved for 4400 timesteps\n",
      "Model saved for 4500 timesteps\n",
      "Model saved for 4600 timesteps\n",
      "Model saved for 4700 timesteps\n",
      "Model saved for 4800 timesteps\n",
      "Model saved for 4900 timesteps\n",
      "Model saved for 5000 timesteps\n",
      "Model saved for 5100 timesteps\n",
      "Model saved for 5200 timesteps\n",
      "Model saved for 5300 timesteps\n",
      "Model saved for 5400 timesteps\n",
      "Model saved for 5500 timesteps\n",
      "Model saved for 5600 timesteps\n",
      "Model saved for 5700 timesteps\n",
      "Model saved for 5800 timesteps\n",
      "Model saved for 5900 timesteps\n",
      "Model saved for 6000 timesteps\n",
      "Model saved for 6100 timesteps\n",
      "Model saved for 6200 timesteps\n",
      "Model saved for 6300 timesteps\n",
      "Model saved for 6400 timesteps\n",
      "Model saved for 6500 timesteps\n",
      "Model saved for 6600 timesteps\n",
      "Model saved for 6700 timesteps\n",
      "Model saved for 6800 timesteps\n",
      "Model saved for 6900 timesteps\n",
      "Model saved for 7000 timesteps\n",
      "Model saved for 7100 timesteps\n",
      "Model saved for 7200 timesteps\n",
      "Model saved for 7300 timesteps\n",
      "Model saved for 7400 timesteps\n",
      "Model saved for 7500 timesteps\n",
      "Model saved for 7600 timesteps\n",
      "Model saved for 7700 timesteps\n",
      "Model saved for 7800 timesteps\n",
      "Model saved for 7900 timesteps\n",
      "Model saved for 8000 timesteps\n",
      "Model saved for 8100 timesteps\n",
      "Model saved for 8200 timesteps\n",
      "Model saved for 8300 timesteps\n",
      "Model saved for 8400 timesteps\n",
      "Model saved for 8500 timesteps\n",
      "Model saved for 8600 timesteps\n",
      "Model saved for 8700 timesteps\n",
      "Model saved for 8800 timesteps\n",
      "Model saved for 8900 timesteps\n",
      "Model saved for 9000 timesteps\n",
      "Model saved for 9100 timesteps\n",
      "Model saved for 9200 timesteps\n",
      "Model saved for 9300 timesteps\n",
      "Model saved for 9400 timesteps\n",
      "Model saved for 9500 timesteps\n",
      "Model saved for 9600 timesteps\n",
      "Model saved for 9700 timesteps\n",
      "Model saved for 9800 timesteps\n",
      "Model saved for 9900 timesteps\n",
      "Model saved for 10000 timesteps\n",
      "Model saved for 10100 timesteps\n",
      "Model saved for 10200 timesteps\n",
      "Model saved for 10300 timesteps\n",
      "Model saved for 10400 timesteps\n",
      "Model saved for 10500 timesteps\n",
      "Model saved for 10600 timesteps\n",
      "Model saved for 10700 timesteps\n",
      "Model saved for 10800 timesteps\n",
      "Model saved for 10900 timesteps\n",
      "Model saved for 11000 timesteps\n",
      "Model saved for 11100 timesteps\n",
      "Model saved for 11200 timesteps\n",
      "Model saved for 11300 timesteps\n",
      "Model saved for 11400 timesteps\n",
      "Model saved for 11500 timesteps\n",
      "Model saved for 11600 timesteps\n",
      "Model saved for 11700 timesteps\n",
      "Model saved for 11800 timesteps\n",
      "Model saved for 11900 timesteps\n",
      "Model saved for 12000 timesteps\n",
      "Model saved for 12100 timesteps\n",
      "Model saved for 12200 timesteps\n",
      "Model saved for 12300 timesteps\n",
      "Model saved for 12400 timesteps\n",
      "Model saved for 12500 timesteps\n",
      "Model saved for 12600 timesteps\n",
      "Model saved for 12700 timesteps\n",
      "Model saved for 12800 timesteps\n",
      "Model saved for 12900 timesteps\n",
      "Model saved for 13000 timesteps\n",
      "Model saved for 13100 timesteps\n",
      "Model saved for 13200 timesteps\n",
      "Model saved for 13300 timesteps\n",
      "Model saved for 13400 timesteps\n",
      "Model saved for 13500 timesteps\n",
      "Model saved for 13600 timesteps\n",
      "Model saved for 13700 timesteps\n",
      "Model saved for 13800 timesteps\n",
      "Model saved for 13900 timesteps\n",
      "Model saved for 14000 timesteps\n",
      "Model saved for 14100 timesteps\n",
      "Model saved for 14200 timesteps\n",
      "Model saved for 14300 timesteps\n",
      "Model saved for 14400 timesteps\n",
      "Model saved for 14500 timesteps\n",
      "Model saved for 14600 timesteps\n",
      "Model saved for 14700 timesteps\n",
      "Model saved for 14800 timesteps\n",
      "Model saved for 14900 timesteps\n",
      "Model saved for 15000 timesteps\n",
      "Model saved for 15100 timesteps\n",
      "Model saved for 15200 timesteps\n",
      "Model saved for 15300 timesteps\n",
      "Model saved for 15400 timesteps\n",
      "Model saved for 15500 timesteps\n",
      "Model saved for 15600 timesteps\n",
      "Model saved for 15700 timesteps\n",
      "Model saved for 15800 timesteps\n",
      "Model saved for 15900 timesteps\n",
      "Model saved for 16000 timesteps\n",
      "Model saved for 16100 timesteps\n",
      "Model saved for 16200 timesteps\n",
      "Model saved for 16300 timesteps\n",
      "Model saved for 16400 timesteps\n",
      "Model saved for 16500 timesteps\n",
      "Model saved for 16600 timesteps\n",
      "Model saved for 16700 timesteps\n",
      "Model saved for 16800 timesteps\n",
      "Model saved for 16900 timesteps\n",
      "Model saved for 17000 timesteps\n",
      "Model saved for 17100 timesteps\n",
      "Model saved for 17200 timesteps\n",
      "Model saved for 17300 timesteps\n",
      "Model saved for 17400 timesteps\n",
      "Model saved for 17500 timesteps\n",
      "Model saved for 17600 timesteps\n",
      "Model saved for 17700 timesteps\n",
      "Model saved for 17800 timesteps\n",
      "Model saved for 17900 timesteps\n",
      "Model saved for 18000 timesteps\n",
      "Model saved for 18100 timesteps\n",
      "Model saved for 18200 timesteps\n",
      "Model saved for 18300 timesteps\n",
      "Model saved for 18400 timesteps\n",
      "Model saved for 18500 timesteps\n",
      "Model saved for 18600 timesteps\n",
      "Model saved for 18700 timesteps\n",
      "Model saved for 18800 timesteps\n",
      "Model saved for 18900 timesteps\n",
      "Model saved for 19000 timesteps\n",
      "Model saved for 19100 timesteps\n",
      "Model saved for 19200 timesteps\n",
      "Model saved for 19300 timesteps\n",
      "Model saved for 19400 timesteps\n",
      "Model saved for 19500 timesteps\n",
      "Model saved for 19600 timesteps\n",
      "Model saved for 19700 timesteps\n",
      "Model saved for 19800 timesteps\n",
      "Model saved for 19900 timesteps\n",
      "Model saved for 20000 timesteps\n",
      "Model saved for 20100 timesteps\n",
      "Model saved for 20200 timesteps\n",
      "Model saved for 20300 timesteps\n",
      "Model saved for 20400 timesteps\n",
      "Model saved for 20500 timesteps\n",
      "Model saved for 20600 timesteps\n",
      "Model saved for 20700 timesteps\n",
      "Model saved for 20800 timesteps\n",
      "Model saved for 20900 timesteps\n",
      "Model saved for 21000 timesteps\n",
      "Model saved for 21100 timesteps\n",
      "Model saved for 21200 timesteps\n",
      "Model saved for 21300 timesteps\n",
      "Model saved for 21400 timesteps\n",
      "Model saved for 21500 timesteps\n",
      "Model saved for 21600 timesteps\n",
      "Model saved for 21700 timesteps\n",
      "Model saved for 21800 timesteps\n",
      "Model saved for 21900 timesteps\n",
      "Model saved for 22000 timesteps\n",
      "Model saved for 22100 timesteps\n",
      "Model saved for 22200 timesteps\n",
      "Model saved for 22300 timesteps\n",
      "Model saved for 22400 timesteps\n",
      "Model saved for 22500 timesteps\n",
      "Model saved for 22600 timesteps\n",
      "Model saved for 22700 timesteps\n",
      "Model saved for 22800 timesteps\n",
      "Model saved for 22900 timesteps\n",
      "Model saved for 23000 timesteps\n",
      "Model saved for 23100 timesteps\n",
      "Model saved for 23200 timesteps\n",
      "Model saved for 23300 timesteps\n",
      "Model saved for 23400 timesteps\n",
      "Model saved for 23500 timesteps\n",
      "Model saved for 23600 timesteps\n",
      "Model saved for 23700 timesteps\n",
      "Model saved for 23800 timesteps\n",
      "Model saved for 23900 timesteps\n",
      "Model saved for 24000 timesteps\n",
      "Model saved for 24100 timesteps\n",
      "Model saved for 24200 timesteps\n",
      "Model saved for 24300 timesteps\n",
      "Model saved for 24400 timesteps\n",
      "Model saved for 24500 timesteps\n",
      "Model saved for 24600 timesteps\n",
      "Model saved for 24700 timesteps\n",
      "Model saved for 24800 timesteps\n",
      "Model saved for 24900 timesteps\n",
      "Model saved for 25000 timesteps\n",
      "Model saved for 25100 timesteps\n",
      "Model saved for 25200 timesteps\n",
      "Model saved for 25300 timesteps\n",
      "Model saved for 25400 timesteps\n",
      "Model saved for 25500 timesteps\n",
      "Model saved for 25600 timesteps\n",
      "Model saved for 25700 timesteps\n",
      "Model saved for 25800 timesteps\n",
      "Model saved for 25900 timesteps\n",
      "Model saved for 26000 timesteps\n",
      "Model saved for 26100 timesteps\n",
      "Model saved for 26200 timesteps\n",
      "Model saved for 26300 timesteps\n",
      "Model saved for 26400 timesteps\n",
      "Model saved for 26500 timesteps\n",
      "Model saved for 26600 timesteps\n",
      "Model saved for 26700 timesteps\n",
      "Model saved for 26800 timesteps\n",
      "Model saved for 26900 timesteps\n",
      "Model saved for 27000 timesteps\n",
      "Model saved for 27100 timesteps\n",
      "Model saved for 27200 timesteps\n",
      "Model saved for 27300 timesteps\n",
      "Model saved for 27400 timesteps\n",
      "Model saved for 27500 timesteps\n",
      "Model saved for 27600 timesteps\n",
      "Model saved for 27700 timesteps\n",
      "Model saved for 27800 timesteps\n",
      "Model saved for 27900 timesteps\n",
      "Model saved for 28000 timesteps\n",
      "Model saved for 28100 timesteps\n",
      "Model saved for 28200 timesteps\n",
      "Model saved for 28300 timesteps\n",
      "Model saved for 28400 timesteps\n",
      "Model saved for 28500 timesteps\n",
      "Model saved for 28600 timesteps\n",
      "Model saved for 28700 timesteps\n",
      "Model saved for 28800 timesteps\n",
      "Model saved for 28900 timesteps\n",
      "Model saved for 29000 timesteps\n",
      "Model saved for 29100 timesteps\n",
      "Model saved for 29200 timesteps\n",
      "Model saved for 29300 timesteps\n",
      "Model saved for 29400 timesteps\n",
      "Model saved for 29500 timesteps\n",
      "Model saved for 29600 timesteps\n",
      "Model saved for 29700 timesteps\n",
      "Model saved for 29800 timesteps\n",
      "Model saved for 29900 timesteps\n",
      "Model saved for 30000 timesteps\n",
      "Model saved for 30100 timesteps\n",
      "Model saved for 30200 timesteps\n",
      "Model saved for 30300 timesteps\n",
      "Model saved for 30400 timesteps\n",
      "Model saved for 30500 timesteps\n",
      "Model saved for 30600 timesteps\n",
      "Model saved for 30700 timesteps\n",
      "Model saved for 30800 timesteps\n",
      "Model saved for 30900 timesteps\n",
      "Model saved for 31000 timesteps\n",
      "Model saved for 31100 timesteps\n",
      "Model saved for 31200 timesteps\n",
      "Model saved for 31300 timesteps\n",
      "Model saved for 31400 timesteps\n",
      "Model saved for 31500 timesteps\n",
      "Model saved for 31600 timesteps\n",
      "Model saved for 31700 timesteps\n",
      "Model saved for 31800 timesteps\n",
      "Model saved for 31900 timesteps\n",
      "Model saved for 32000 timesteps\n",
      "Model saved for 32100 timesteps\n",
      "Model saved for 32200 timesteps\n",
      "Model saved for 32300 timesteps\n",
      "Model saved for 32400 timesteps\n",
      "Model saved for 32500 timesteps\n",
      "Model saved for 32600 timesteps\n",
      "Model saved for 32700 timesteps\n",
      "Model saved for 32800 timesteps\n",
      "Model saved for 32900 timesteps\n",
      "Model saved for 33000 timesteps\n",
      "Model saved for 33100 timesteps\n",
      "Model saved for 33200 timesteps\n",
      "Model saved for 33300 timesteps\n",
      "Model saved for 33400 timesteps\n",
      "Model saved for 33500 timesteps\n",
      "Model saved for 33600 timesteps\n",
      "Model saved for 33700 timesteps\n",
      "Model saved for 33800 timesteps\n",
      "Model saved for 33900 timesteps\n",
      "Model saved for 34000 timesteps\n",
      "Model saved for 34100 timesteps\n",
      "Model saved for 34200 timesteps\n",
      "Model saved for 34300 timesteps\n",
      "Model saved for 34400 timesteps\n",
      "Model saved for 34500 timesteps\n",
      "Model saved for 34600 timesteps\n",
      "Model saved for 34700 timesteps\n",
      "Model saved for 34800 timesteps\n",
      "Model saved for 34900 timesteps\n",
      "Model saved for 35000 timesteps\n",
      "Model saved for 35100 timesteps\n",
      "Model saved for 35200 timesteps\n",
      "Model saved for 35300 timesteps\n",
      "Model saved for 35400 timesteps\n",
      "Model saved for 35500 timesteps\n",
      "Model saved for 35600 timesteps\n",
      "Model saved for 35700 timesteps\n",
      "Model saved for 35800 timesteps\n",
      "Model saved for 35900 timesteps\n",
      "Model saved for 36000 timesteps\n",
      "Model saved for 36100 timesteps\n",
      "Model saved for 36200 timesteps\n",
      "Model saved for 36300 timesteps\n",
      "Model saved for 36400 timesteps\n",
      "Model saved for 36500 timesteps\n",
      "Model saved for 36600 timesteps\n",
      "Model saved for 36700 timesteps\n",
      "Model saved for 36800 timesteps\n",
      "Model saved for 36900 timesteps\n",
      "Model saved for 37000 timesteps\n",
      "Model saved for 37100 timesteps\n",
      "Model saved for 37200 timesteps\n",
      "Model saved for 37300 timesteps\n",
      "Model saved for 37400 timesteps\n",
      "Model saved for 37500 timesteps\n",
      "Model saved for 37600 timesteps\n",
      "Model saved for 37700 timesteps\n",
      "Model saved for 37800 timesteps\n",
      "Model saved for 37900 timesteps\n",
      "Model saved for 38000 timesteps\n",
      "Model saved for 38100 timesteps\n",
      "Model saved for 38200 timesteps\n",
      "Model saved for 38300 timesteps\n",
      "Model saved for 38400 timesteps\n",
      "Model saved for 38500 timesteps\n",
      "Model saved for 38600 timesteps\n",
      "Model saved for 38700 timesteps\n",
      "Model saved for 38800 timesteps\n",
      "Model saved for 38900 timesteps\n",
      "Model saved for 39000 timesteps\n",
      "Model saved for 39100 timesteps\n",
      "Model saved for 39200 timesteps\n",
      "Model saved for 39300 timesteps\n",
      "Model saved for 39400 timesteps\n",
      "Model saved for 39500 timesteps\n",
      "Model saved for 39600 timesteps\n",
      "Model saved for 39700 timesteps\n",
      "Model saved for 39800 timesteps\n",
      "Model saved for 39900 timesteps\n",
      "Model saved for 40000 timesteps\n",
      "Model saved for 40100 timesteps\n",
      "Model saved for 40200 timesteps\n",
      "Model saved for 40300 timesteps\n",
      "Model saved for 40400 timesteps\n",
      "Model saved for 40500 timesteps\n",
      "Model saved for 40600 timesteps\n",
      "Model saved for 40700 timesteps\n",
      "Model saved for 40800 timesteps\n",
      "Model saved for 40900 timesteps\n",
      "Model saved for 41000 timesteps\n",
      "Model saved for 41100 timesteps\n",
      "Model saved for 41200 timesteps\n",
      "Model saved for 41300 timesteps\n",
      "Model saved for 41400 timesteps\n",
      "Model saved for 41500 timesteps\n",
      "Model saved for 41600 timesteps\n",
      "Model saved for 41700 timesteps\n",
      "Model saved for 41800 timesteps\n",
      "Model saved for 41900 timesteps\n",
      "Model saved for 42000 timesteps\n",
      "Model saved for 42100 timesteps\n",
      "Model saved for 42200 timesteps\n",
      "Model saved for 42300 timesteps\n",
      "Model saved for 42400 timesteps\n",
      "Model saved for 42500 timesteps\n",
      "Model saved for 42600 timesteps\n",
      "Model saved for 42700 timesteps\n",
      "Model saved for 42800 timesteps\n",
      "Model saved for 42900 timesteps\n",
      "Model saved for 43000 timesteps\n",
      "Model saved for 43100 timesteps\n",
      "Model saved for 43200 timesteps\n",
      "Model saved for 43300 timesteps\n",
      "Model saved for 43400 timesteps\n",
      "Model saved for 43500 timesteps\n",
      "Model saved for 43600 timesteps\n",
      "Model saved for 43700 timesteps\n",
      "Model saved for 43800 timesteps\n",
      "Model saved for 43900 timesteps\n",
      "Model saved for 44000 timesteps\n",
      "Model saved for 44100 timesteps\n",
      "Model saved for 44200 timesteps\n",
      "Model saved for 44300 timesteps\n",
      "Model saved for 44400 timesteps\n",
      "Model saved for 44500 timesteps\n",
      "Model saved for 44600 timesteps\n",
      "Model saved for 44700 timesteps\n",
      "Model saved for 44800 timesteps\n",
      "Model saved for 44900 timesteps\n",
      "Model saved for 45000 timesteps\n",
      "Model saved for 45100 timesteps\n",
      "Model saved for 45200 timesteps\n",
      "Model saved for 45300 timesteps\n",
      "Model saved for 45400 timesteps\n",
      "Model saved for 45500 timesteps\n",
      "Model saved for 45600 timesteps\n",
      "Model saved for 45700 timesteps\n",
      "Model saved for 45800 timesteps\n",
      "Model saved for 45900 timesteps\n",
      "Model saved for 46000 timesteps\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "This block defines and trains a Neural Network.\n",
    "\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# define a function which applies xavier initialization to the weights of the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "for timesteps in list(range(100, 50100, 100)):\n",
    "    # initialize the network and apply xavier initialization\n",
    "    model = Net()\n",
    "\n",
    "    # apply xavier initialization to the weights of the network\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    _ = env.reset()\n",
    "    state = env.state.copy()\n",
    "    done = False\n",
    "    for t in range(timesteps):\n",
    "        action = np.random.uniform(low=-2.0, high=2.0)\n",
    "        state_action = np.append(state, action)\n",
    "        s, reward, done, _ = env.step([action])\n",
    "        if done:\n",
    "            _ = env.reset()\n",
    "            done = False\n",
    "        next_state = env.state.copy()\n",
    "        prediction = model(torch.tensor(state_action, dtype=torch.float32))\n",
    "        loss = criterion(prediction, torch.tensor(next_state, dtype=torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        state = next_state\n",
    "\n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), 'NN_models/NN_{}.pt'.format(timesteps))\n",
    "    print('Model saved for {} timesteps'.format(timesteps))\n",
    "    \n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This block defines the MPPI class.\n",
    "\n",
    "'''\n",
    "\n",
    "import functools\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _ensure_non_zero(cost, beta, factor):\n",
    "    return torch.exp(-factor * (cost - beta))\n",
    "\n",
    "\n",
    "def is_tensor_like(x):\n",
    "    return torch.is_tensor(x) or type(x) is np.ndarray\n",
    "\n",
    "\n",
    "def squeeze_n(v, n_squeeze):\n",
    "    for _ in range(n_squeeze):\n",
    "        v = v.squeeze(0)\n",
    "    return v\n",
    "\n",
    "\n",
    "# from arm_pytorch_utilities, standalone since that package is not on pypi yet\n",
    "def handle_batch_input(n):\n",
    "    def _handle_batch_input(func):\n",
    "        \"\"\"For func that expect 2D input, handle input that have more than 2 dimensions by flattening them temporarily\"\"\"\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # assume inputs that are tensor-like have compatible shapes and is represented by the first argument\n",
    "            batch_dims = []\n",
    "            for arg in args:\n",
    "                if is_tensor_like(arg):\n",
    "                    if len(arg.shape) > n:\n",
    "                        # last dimension is type dependent; all previous ones are batches\n",
    "                        batch_dims = arg.shape[:-(n - 1)]\n",
    "                        break\n",
    "                    elif len(arg.shape) < n:\n",
    "                        n_batch_dims_to_add = n - len(arg.shape)\n",
    "                        batch_ones_to_add = [1] * n_batch_dims_to_add\n",
    "                        args = [v.view(*batch_ones_to_add, *v.shape) if is_tensor_like(v) else v for v in args]\n",
    "                        ret = func(*args, **kwargs)\n",
    "                        if isinstance(ret, tuple):\n",
    "                            ret = [squeeze_n(v, n_batch_dims_to_add) if is_tensor_like(v) else v for v in ret]\n",
    "                            return ret\n",
    "                        else:\n",
    "                            if is_tensor_like(ret):\n",
    "                                return squeeze_n(ret, n_batch_dims_to_add)\n",
    "                            else:\n",
    "                                return ret\n",
    "            # no batches; just return normally\n",
    "            if not batch_dims:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # reduce all batch dimensions down to the first one\n",
    "            args = [v.view(-1, *v.shape[-(n - 1):]) if (is_tensor_like(v) and len(v.shape) > 2) else v for v in args]\n",
    "            ret = func(*args, **kwargs)\n",
    "            # restore original batch dimensions; keep variable dimension (nx)\n",
    "            if type(ret) is tuple:\n",
    "                ret = [v if (not is_tensor_like(v) or len(v.shape) == 0) else (\n",
    "                    v.view(*batch_dims, *v.shape[-(n - 1):]) if len(v.shape) == n else v.view(*batch_dims)) for v in\n",
    "                       ret]\n",
    "            else:\n",
    "                if is_tensor_like(ret):\n",
    "                    if len(ret.shape) == n:\n",
    "                        ret = ret.view(*batch_dims, *ret.shape[-(n - 1):])\n",
    "                    else:\n",
    "                        ret = ret.view(*batch_dims)\n",
    "            return ret\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _handle_batch_input\n",
    "\n",
    "\n",
    "class MPPI():\n",
    "    \"\"\"\n",
    "    Model Predictive Path Integral control\n",
    "    This implementation batch samples the trajectories and so scales well with the number of samples K.\n",
    "\n",
    "    Implemented according to algorithm 2 in Williams et al., 2017\n",
    "    'Information Theoretic MPC for Model-Based Reinforcement Learning',\n",
    "    based off of https://github.com/ferreirafabio/mppi_pendulum\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dynamics, running_cost, nx, noise_sigma, num_samples=100, horizon=15, device=\"cpu\",\n",
    "                 terminal_state_cost=None,\n",
    "                 lambda_=1.,\n",
    "                 noise_mu=None,\n",
    "                 u_min=None,\n",
    "                 u_max=None,\n",
    "                 u_init=None,\n",
    "                 U_init=None,\n",
    "                 u_scale=1,\n",
    "                 u_per_command=1,\n",
    "                 step_dependent_dynamics=False,\n",
    "                 rollout_samples=1,\n",
    "                 rollout_var_cost=0,\n",
    "                 rollout_var_discount=0.95,\n",
    "                 sample_null_action=False,\n",
    "                 noise_abs_cost=False):\n",
    "        \"\"\"\n",
    "        :param dynamics: function(state, action) -> next_state (K x nx) taking in batch state (K x nx) and action (K x nu)\n",
    "        :param running_cost: function(state, action) -> cost (K) taking in batch state and action (same as dynamics)\n",
    "        :param nx: state dimension\n",
    "        :param noise_sigma: (nu x nu) control noise covariance (assume v_t ~ N(u_t, noise_sigma))\n",
    "        :param num_samples: K, number of trajectories to sample\n",
    "        :param horizon: T, length of each trajectory\n",
    "        :param device: pytorch device\n",
    "        :param terminal_state_cost: function(state) -> cost (K x 1) taking in batch state\n",
    "        :param lambda_: temperature, positive scalar where larger values will allow more exploration\n",
    "        :param noise_mu: (nu) control noise mean (used to bias control samples); defaults to zero mean\n",
    "        :param u_min: (nu) minimum values for each dimension of control to pass into dynamics\n",
    "        :param u_max: (nu) maximum values for each dimension of control to pass into dynamics\n",
    "        :param u_init: (nu) what to initialize new end of trajectory control to be; defeaults to zero\n",
    "        :param U_init: (T x nu) initial control sequence; defaults to noise\n",
    "        :param step_dependent_dynamics: whether the passed in dynamics needs horizon step passed in (as 3rd arg)\n",
    "        :param rollout_samples: M, number of state trajectories to rollout for each control trajectory\n",
    "            (should be 1 for deterministic dynamics and more for models that output a distribution)\n",
    "        :param rollout_var_cost: Cost attached to the variance of costs across trajectory rollouts\n",
    "        :param rollout_var_discount: Discount of variance cost over control horizon\n",
    "        :param sample_null_action: Whether to explicitly sample a null action (bad for starting in a local minima)\n",
    "        :param noise_abs_cost: Whether to use the absolute value of the action noise to avoid bias when all states have the same cost\n",
    "        \"\"\"\n",
    "        self.d = device\n",
    "        self.dtype = noise_sigma.dtype\n",
    "        self.K = num_samples  # N_SAMPLES\n",
    "        self.T = horizon  # TIMESTEPS\n",
    "\n",
    "        # dimensions of state and control\n",
    "        self.nx = nx\n",
    "        self.nu = 1 if len(noise_sigma.shape) == 0 else noise_sigma.shape[0]\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        if noise_mu is None:\n",
    "            noise_mu = torch.zeros(self.nu, dtype=self.dtype)\n",
    "\n",
    "        if u_init is None:\n",
    "            u_init = torch.zeros_like(noise_mu)\n",
    "\n",
    "        # handle 1D edge case\n",
    "        if self.nu == 1:\n",
    "            noise_mu = noise_mu.view(-1)\n",
    "            noise_sigma = noise_sigma.view(-1, 1)\n",
    "\n",
    "        # bounds\n",
    "        self.u_min = u_min\n",
    "        self.u_max = u_max\n",
    "        self.u_scale = u_scale\n",
    "        self.u_per_command = u_per_command\n",
    "        # make sure if any of them is specified, both are specified\n",
    "        if self.u_max is not None and self.u_min is None:\n",
    "            if not torch.is_tensor(self.u_max):\n",
    "                self.u_max = torch.tensor(self.u_max)\n",
    "            self.u_min = -self.u_max\n",
    "        if self.u_min is not None and self.u_max is None:\n",
    "            if not torch.is_tensor(self.u_min):\n",
    "                self.u_min = torch.tensor(self.u_min)\n",
    "            self.u_max = -self.u_min\n",
    "        if self.u_min is not None:\n",
    "            self.u_min = self.u_min.to(device=self.d)\n",
    "            self.u_max = self.u_max.to(device=self.d)\n",
    "\n",
    "        self.noise_mu = noise_mu.to(self.d)\n",
    "        self.noise_sigma = noise_sigma.to(self.d)\n",
    "        self.noise_sigma_inv = torch.inverse(self.noise_sigma)\n",
    "        self.noise_dist = MultivariateNormal(self.noise_mu, covariance_matrix=self.noise_sigma)\n",
    "        # T x nu control sequence\n",
    "        self.U = U_init\n",
    "        self.u_init = u_init.to(self.d)\n",
    "\n",
    "        if self.U is None:\n",
    "            self.U = self.noise_dist.sample((self.T,))\n",
    "\n",
    "        self.step_dependency = step_dependent_dynamics\n",
    "        self.F = dynamics\n",
    "        self.running_cost = running_cost\n",
    "        self.terminal_state_cost = terminal_state_cost\n",
    "        self.sample_null_action = sample_null_action\n",
    "        self.noise_abs_cost = noise_abs_cost\n",
    "        self.state = None\n",
    "\n",
    "        # handling dynamics models that output a distribution (take multiple trajectory samples)\n",
    "        self.M = rollout_samples\n",
    "        self.rollout_var_cost = rollout_var_cost\n",
    "        self.rollout_var_discount = rollout_var_discount\n",
    "\n",
    "        # sampled results from last command\n",
    "        self.cost_total = None\n",
    "        self.cost_total_non_zero = None\n",
    "        self.omega = None\n",
    "        self.states = None\n",
    "        self.actions = None\n",
    "\n",
    "    @handle_batch_input(n=2)\n",
    "    def _dynamics(self, state, u, t):\n",
    "        return self.F(state, u, t) if self.step_dependency else self.F(state, u)\n",
    "\n",
    "    @handle_batch_input(n=2)\n",
    "    def _running_cost(self, state, u):\n",
    "        return self.running_cost(state, u)\n",
    "\n",
    "    def command(self, state, shift_nominal_trajectory=True):\n",
    "        \"\"\"\n",
    "        :param state: (nx) or (K x nx) current state, or samples of states (for propagating a distribution of states)\n",
    "        :param shift_nominal_trajectory: Whether to roll the nominal trajectory forward one step. This should be True\n",
    "        if the command is to be executed. If the nominal trajectory is to be refined then it should be False.\n",
    "        :returns action: (nu) best action\n",
    "        \"\"\"\n",
    "        if shift_nominal_trajectory:\n",
    "            # shift command 1 time step\n",
    "            self.U = torch.roll(self.U, -1, dims=0)\n",
    "            self.U[-1] = self.u_init\n",
    "\n",
    "        return self._command(state)\n",
    "\n",
    "    def _command(self, state):\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state)\n",
    "        self.state = state.to(dtype=self.dtype, device=self.d)\n",
    "        cost_total = self._compute_total_cost_batch()\n",
    "        beta = torch.min(cost_total)\n",
    "        self.cost_total_non_zero = _ensure_non_zero(cost_total, beta, 1 / self.lambda_)\n",
    "        eta = torch.sum(self.cost_total_non_zero)\n",
    "        self.omega = (1. / eta) * self.cost_total_non_zero\n",
    "        perturbations = []\n",
    "        for t in range(self.T):\n",
    "            perturbations.append(torch.sum(self.omega.view(-1, 1) * self.noise[:, t], dim=0))\n",
    "        perturbations = torch.stack(perturbations)\n",
    "        self.U = self.U + perturbations\n",
    "        action = self.U[:self.u_per_command]\n",
    "        # reduce dimensionality if we only need the first command\n",
    "        if self.u_per_command == 1:\n",
    "            action = action[0]\n",
    "        return action\n",
    "\n",
    "    def change_horizon(self, horizon):\n",
    "        if horizon < self.U.shape[0]:\n",
    "            # truncate trajectory\n",
    "            self.U = self.U[:horizon]\n",
    "        elif horizon > self.U.shape[0]:\n",
    "            # extend with u_init\n",
    "            self.U = torch.cat((self.U, self.u_init.repeat(horizon - self.U.shape[0], 1)))\n",
    "        self.T = horizon\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Clear controller state after finishing a trial\n",
    "        \"\"\"\n",
    "        self.U = self.noise_dist.sample((self.T,))\n",
    "\n",
    "    def _compute_rollout_costs(self, perturbed_actions):\n",
    "        K, T, nu = perturbed_actions.shape\n",
    "        assert nu == self.nu\n",
    "\n",
    "        cost_total = torch.zeros(K, device=self.d, dtype=self.dtype)\n",
    "        cost_samples = cost_total.repeat(self.M, 1)\n",
    "        cost_var = torch.zeros_like(cost_total)\n",
    "\n",
    "        # allow propagation of a sample of states (ex. to carry a distribution), or to start with a single state\n",
    "        if self.state.shape == (K, self.nx):\n",
    "            state = self.state\n",
    "        else:\n",
    "            state = self.state.view(1, -1).repeat(K, 1)\n",
    "\n",
    "        # rollout action trajectory M times to estimate expected cost\n",
    "        state = state.repeat(self.M, 1, 1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        for t in range(T):\n",
    "            u = self.u_scale * perturbed_actions[:, t].repeat(self.M, 1, 1)\n",
    "            state = self._dynamics(state, u, t)\n",
    "            c = self._running_cost(state, u)\n",
    "            cost_samples = cost_samples + c\n",
    "            if self.M > 1:\n",
    "                cost_var += c.var(dim=0) * (self.rollout_var_discount ** t)\n",
    "\n",
    "            # Save total states/actions\n",
    "            states.append(state)\n",
    "            actions.append(u)\n",
    "\n",
    "        # Actions is K x T x nu\n",
    "        # States is K x T x nx\n",
    "        actions = torch.stack(actions, dim=-2)\n",
    "        states = torch.stack(states, dim=-2)\n",
    "\n",
    "        # action perturbation cost\n",
    "        if self.terminal_state_cost:\n",
    "            c = self.terminal_state_cost(states, actions)\n",
    "            cost_samples = cost_samples + c\n",
    "        cost_total = cost_total + cost_samples.mean(dim=0)\n",
    "        cost_total = cost_total + cost_var * self.rollout_var_cost\n",
    "        return cost_total, states, actions\n",
    "\n",
    "    def _compute_total_cost_batch(self):\n",
    "        # parallelize sampling across trajectories\n",
    "        # resample noise each time we take an action\n",
    "        noise = self.noise_dist.rsample((self.K, self.T))\n",
    "        # broadcast own control to noise over samples; now it's K x T x nu\n",
    "        perturbed_action = self.U + noise\n",
    "        if self.sample_null_action:\n",
    "            perturbed_action[self.K - 1] = 0\n",
    "        # naively bound control\n",
    "        self.perturbed_action = self._bound_action(perturbed_action)\n",
    "        # bounded noise after bounding (some got cut off, so we don't penalize that in action cost)\n",
    "        self.noise = self.perturbed_action - self.U\n",
    "        if self.noise_abs_cost:\n",
    "            action_cost = self.lambda_ * torch.abs(self.noise) @ self.noise_sigma_inv\n",
    "            # NOTE: The original paper does self.lambda_ * torch.abs(self.noise) @ self.noise_sigma_inv, but this biases\n",
    "            # the actions with low noise if all states have the same cost. With abs(noise) we prefer actions close to the\n",
    "            # nomial trajectory.\n",
    "        else:\n",
    "            action_cost = self.lambda_ * self.noise @ self.noise_sigma_inv  # Like original paper\n",
    "\n",
    "        rollout_cost, self.states, actions = self._compute_rollout_costs(self.perturbed_action)\n",
    "        self.actions = actions / self.u_scale\n",
    "\n",
    "        # action perturbation cost\n",
    "        perturbation_cost = torch.sum(self.U * action_cost, dim=(1, 2))\n",
    "        self.cost_total = rollout_cost + perturbation_cost\n",
    "        return self.cost_total\n",
    "\n",
    "    def _bound_action(self, action):\n",
    "        if self.u_max is not None:\n",
    "            for t in range(self.T):\n",
    "                u = action[:, self._slice_control(t)]\n",
    "                cu = torch.max(torch.min(u, self.u_max), self.u_min)\n",
    "                action[:, self._slice_control(t)] = cu\n",
    "        return action\n",
    "\n",
    "    def _slice_control(self, t):\n",
    "        return slice(t * self.nu, (t + 1) * self.nu)\n",
    "\n",
    "    def get_rollouts(self, state, num_rollouts=1):\n",
    "        \"\"\"\n",
    "            :param state: either (nx) vector or (num_rollouts x nx) for sampled initial states\n",
    "            :param num_rollouts: Number of rollouts with same action sequence - for generating samples with stochastic\n",
    "                                 dynamics\n",
    "            :returns states: num_rollouts x T x nx vector of trajectories\n",
    "\n",
    "        \"\"\"\n",
    "        state = state.view(-1, self.nx)\n",
    "        if state.size(0) == 1:\n",
    "            state = state.repeat(num_rollouts, 1)\n",
    "\n",
    "        T = self.U.shape[0]\n",
    "        states = torch.zeros((num_rollouts, T + 1, self.nx), dtype=self.U.dtype, device=self.U.device)\n",
    "        states[:, 0] = state\n",
    "        for t in range(T):\n",
    "            states[:, t + 1] = self._dynamics(states[:, t].view(num_rollouts, -1),\n",
    "                                              self.u_scale * self.U[t].view(num_rollouts, -1), t)\n",
    "        return states[:, 1:]\n",
    "\n",
    "\n",
    "def run_mppi(mppi, env, retrain_dynamics, retrain_after_iter=50, iter=1000, render=True):\n",
    "    dataset = torch.zeros((retrain_after_iter, mppi.nx + mppi.nu), dtype=mppi.U.dtype, device=mppi.d)\n",
    "    rewards = []\n",
    "    for i in range(iter):\n",
    "        state = env.state.copy()\n",
    "        command_start = time.perf_counter()\n",
    "        action = mppi.command(state)\n",
    "        elapsed = time.perf_counter() - command_start\n",
    "        s, r, _, _ = env.step(action.detach().numpy())\n",
    "        total_reward += r\n",
    "        rewards.append(r)\n",
    "        # logger.debug(\"action taken: %.4f reward received: %.4f time taken: %.5fs\", action, r, elapsed)\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        di = i % retrain_after_iter\n",
    "        if di == 0 and i > 0:\n",
    "            retrain_dynamics(dataset)\n",
    "            # don't have to clear dataset since it'll be overridden, but useful for debugging\n",
    "            dataset.zero_()\n",
    "        dataset[di, :mppi.nx] = torch.tensor(state, dtype=mppi.U.dtype)\n",
    "        dataset[di, mppi.nx:] = action\n",
    "    return rewards, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This block uses the trained Neural Network and MPPI to control the Pendulum.\n",
    "\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "from gym import wrappers, logger as gym_log\n",
    "\n",
    "gym_log.set_level(gym_log.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='[%(levelname)s %(asctime)s %(pathname)s:%(lineno)d] %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "ENV_NAME = \"Pendulum-v1\"\n",
    "TIMESTEPS = 15  # T\n",
    "N_SAMPLES = 100  # K\n",
    "ACTION_LOW = -2.0\n",
    "ACTION_HIGH = 2.0\n",
    "\n",
    "d = \"cuda\"\n",
    "dtype = torch.double\n",
    "\n",
    "noise_sigma = torch.tensor(10, device=d, dtype=dtype)\n",
    "# noise_sigma = torch.tensor([[10, 0], [0, 10]], device=d, dtype=dtype)\n",
    "lambda_ = 1.\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('NN_models/NN_5000.pt'))\n",
    "\n",
    "def dynamics(state, perturbed_action):\n",
    "    # NN prediction\n",
    "    # join state and action\n",
    "    state_action = torch.cat((state, perturbed_action), dim=1)\n",
    "    \n",
    "    # make state_action type dtype=torch.float32\n",
    "    state_action = state_action.type(dtype=torch.float32)\n",
    "    prediction = model(state_action)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x + math.pi) % (2 * math.pi)) - math.pi)\n",
    "\n",
    "\n",
    "def running_cost(state, action):\n",
    "    theta = state[:, 0]\n",
    "    theta_dt = state[:, 1]\n",
    "    action = action[:, 0]\n",
    "    cost = angle_normalize(theta) ** 2 + 0.1 * theta_dt ** 2\n",
    "    return cost\n",
    "\n",
    "\n",
    "def train(new_data):\n",
    "    pass\n",
    "\n",
    "\n",
    "downward_start = False\n",
    "env = gym.make(ENV_NAME).env  # bypass the default TimeLimit wrapper\n",
    "env.reset()\n",
    "if downward_start:\n",
    "    env.state = [np.pi, 1]\n",
    "nx = 2\n",
    "mppi_gym = MPPI(dynamics, running_cost, nx, noise_sigma, num_samples=N_SAMPLES, horizon=TIMESTEPS,\n",
    "                        lambda_=lambda_)\n",
    "\n",
    "iteration = 10000\n",
    "rewards, dataset = run_mppi(mppi=mppi_gym, env=env, retrain_dynamics=train, retrain_after_iter=50, iter=iteration, render=False)\n",
    "\n",
    "print(\"Average reward: \", sum(rewards) / iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = pd.DataFrame(columns=[\"Timesteps_trained\", \"rewards_mean\", \"rewards_std\", \"rewards_max\", \"rewards_min\", \"rewards_median\", \"rewards_25\", \"rewards_75\"])\n",
    "\n",
    "ENV_NAME = \"Pendulum-v1\"\n",
    "TIMESTEPS = 15  # T\n",
    "N_SAMPLES = 100  # K\n",
    "ACTION_LOW = -2.0\n",
    "ACTION_HIGH = 2.0\n",
    "\n",
    "d = \"cuda\"\n",
    "dtype = torch.double\n",
    "\n",
    "noise_sigma = torch.tensor(10, device=d, dtype=dtype)\n",
    "# noise_sigma = torch.tensor([[10, 0], [0, 10]], device=d, dtype=dtype)\n",
    "lambda_ = 1.\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x + math.pi) % (2 * math.pi)) - math.pi)\n",
    "\n",
    "\n",
    "def running_cost(state, action):\n",
    "    theta = state[:, 0]\n",
    "    theta_dt = state[:, 1]\n",
    "    action = action[:, 0]\n",
    "    cost = angle_normalize(theta) ** 2 + 0.1 * theta_dt ** 2\n",
    "    return cost\n",
    "\n",
    "def train(new_data):\n",
    "    pass\n",
    "\n",
    "for timesteps in list(range(100, 50100, 100)):\n",
    "\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load('NN_models/NN_{}.pt'.format(timesteps)))\n",
    "\n",
    "    def dynamics(state, perturbed_action):\n",
    "        # NN prediction\n",
    "        # join state and action\n",
    "        state_action = torch.cat((state, perturbed_action), dim=1)\n",
    "        \n",
    "        # make state_action type dtype=torch.float32\n",
    "        state_action = state_action.type(dtype=torch.float32)\n",
    "        prediction = model(state_action)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    downward_start = False\n",
    "    env = gym.make(ENV_NAME).env  # bypass the default TimeLimit wrapper\n",
    "    env.reset()\n",
    "    if downward_start:\n",
    "        env.state = [np.pi, 1]\n",
    "    nx = 2\n",
    "    mppi_gym = MPPI(dynamics, running_cost, nx, noise_sigma, num_samples=N_SAMPLES, horizon=TIMESTEPS,\n",
    "                            lambda_=lambda_)\n",
    "\n",
    "    iteration = 10000\n",
    "    rewards, dataset = run_mppi(mppi=mppi_gym, env=env, retrain_dynamics=train, retrain_after_iter=50, iter=iteration, render=False)\n",
    "\n",
    "    # calculate statistics\n",
    "    rewards_mean = np.mean(rewards)\n",
    "    rewards_std = np.std(rewards)\n",
    "    rewards_max = np.max(rewards)\n",
    "    rewards_min = np.min(rewards)\n",
    "    rewards_median = np.median(rewards)\n",
    "    rewards_25 = np.percentile(rewards, 25)\n",
    "    rewards_75 = np.percentile(rewards, 75)\n",
    "    result.loc[len(result)] = [timesteps, rewards_mean, rewards_std, rewards_max, rewards_min, rewards_median, rewards_25, rewards_75]\n",
    "\n",
    "    print(\"Average reward: \", rewards_mean)\n",
    "\n",
    "\n",
    "result.to_csv(\"NN_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhiyu39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "210d99c3fa445973a1a9dbd666f41525256243834c70425bd71fc0f1f877f877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
