{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved for 1000 timesteps\n",
      "Model saved for 11000 timesteps\n",
      "Model saved for 21000 timesteps\n",
      "Model saved for 31000 timesteps\n",
      "Model saved for 41000 timesteps\n",
      "Model saved for 51000 timesteps\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "This block defines and trains a Neural Network.\n",
    "\n",
    "'''\n",
    "\n",
    "FOLDER = 'NN_SGD_No_Activation_0.01'\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# define a function which applies xavier initialization to the weights of the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "for timesteps in list(range(1000, 52000, 10000)):\n",
    "    # initialize the network and apply xavier initialization\n",
    "    model = Net()\n",
    "\n",
    "    # apply xavier initialization to the weights of the network\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    _ = env.reset()\n",
    "    state = env.state.copy()\n",
    "    done = False\n",
    "    for t in range(timesteps):\n",
    "        action = np.random.uniform(low=-2.0, high=2.0)\n",
    "        state_action = np.append(state, action)\n",
    "        s, reward, done, _ = env.step([action])\n",
    "        if done:\n",
    "            _ = env.reset()\n",
    "            done = False\n",
    "        next_state = env.state.copy()\n",
    "        prediction = model(torch.tensor(state_action, dtype=torch.float32))\n",
    "        loss = criterion(prediction, torch.tensor(next_state, dtype=torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        state = next_state\n",
    "\n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), FOLDER+'/NN_{}.pt'.format(timesteps))\n",
    "    print('Model saved for {} timesteps'.format(timesteps))\n",
    "    \n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This block defines the MPPI class.\n",
    "Adam\n",
    "'''\n",
    "\n",
    "import functools\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _ensure_non_zero(cost, beta, factor):\n",
    "    return torch.exp(-factor * (cost - beta))\n",
    "\n",
    "\n",
    "def is_tensor_like(x):\n",
    "    return torch.is_tensor(x) or type(x) is np.ndarray\n",
    "\n",
    "\n",
    "def squeeze_n(v, n_squeeze):\n",
    "    for _ in range(n_squeeze):\n",
    "        v = v.squeeze(0)\n",
    "    return v\n",
    "\n",
    "\n",
    "# from arm_pytorch_utilities, standalone since that package is not on pypi yet\n",
    "def handle_batch_input(n):\n",
    "    def _handle_batch_input(func):\n",
    "        \"\"\"For func that expect 2D input, handle input that have more than 2 dimensions by flattening them temporarily\"\"\"\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # assume inputs that are tensor-like have compatible shapes and is represented by the first argument\n",
    "            batch_dims = []\n",
    "            for arg in args:\n",
    "                if is_tensor_like(arg):\n",
    "                    if len(arg.shape) > n:\n",
    "                        # last dimension is type dependent; all previous ones are batches\n",
    "                        batch_dims = arg.shape[:-(n - 1)]\n",
    "                        break\n",
    "                    elif len(arg.shape) < n:\n",
    "                        n_batch_dims_to_add = n - len(arg.shape)\n",
    "                        batch_ones_to_add = [1] * n_batch_dims_to_add\n",
    "                        args = [v.view(*batch_ones_to_add, *v.shape) if is_tensor_like(v) else v for v in args]\n",
    "                        ret = func(*args, **kwargs)\n",
    "                        if isinstance(ret, tuple):\n",
    "                            ret = [squeeze_n(v, n_batch_dims_to_add) if is_tensor_like(v) else v for v in ret]\n",
    "                            return ret\n",
    "                        else:\n",
    "                            if is_tensor_like(ret):\n",
    "                                return squeeze_n(ret, n_batch_dims_to_add)\n",
    "                            else:\n",
    "                                return ret\n",
    "            # no batches; just return normally\n",
    "            if not batch_dims:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # reduce all batch dimensions down to the first one\n",
    "            args = [v.view(-1, *v.shape[-(n - 1):]) if (is_tensor_like(v) and len(v.shape) > 2) else v for v in args]\n",
    "            ret = func(*args, **kwargs)\n",
    "            # restore original batch dimensions; keep variable dimension (nx)\n",
    "            if type(ret) is tuple:\n",
    "                ret = [v if (not is_tensor_like(v) or len(v.shape) == 0) else (\n",
    "                    v.view(*batch_dims, *v.shape[-(n - 1):]) if len(v.shape) == n else v.view(*batch_dims)) for v in\n",
    "                       ret]\n",
    "            else:\n",
    "                if is_tensor_like(ret):\n",
    "                    if len(ret.shape) == n:\n",
    "                        ret = ret.view(*batch_dims, *ret.shape[-(n - 1):])\n",
    "                    else:\n",
    "                        ret = ret.view(*batch_dims)\n",
    "            return ret\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _handle_batch_input\n",
    "\n",
    "\n",
    "class MPPI():\n",
    "    \"\"\"\n",
    "    Model Predictive Path Integral control\n",
    "    This implementation batch samples the trajectories and so scales well with the number of samples K.\n",
    "\n",
    "    Implemented according to algorithm 2 in Williams et al., 2017\n",
    "    'Information Theoretic MPC for Model-Based Reinforcement Learning',\n",
    "    based off of https://github.com/ferreirafabio/mppi_pendulum\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dynamics, running_cost, nx, noise_sigma, num_samples=100, horizon=15, device=\"cpu\",\n",
    "                 terminal_state_cost=None,\n",
    "                 lambda_=1.,\n",
    "                 noise_mu=None,\n",
    "                 u_min=None,\n",
    "                 u_max=None,\n",
    "                 u_init=None,\n",
    "                 U_init=None,\n",
    "                 u_scale=1,\n",
    "                 u_per_command=1,\n",
    "                 step_dependent_dynamics=False,\n",
    "                 rollout_samples=1,\n",
    "                 rollout_var_cost=0,\n",
    "                 rollout_var_discount=0.95,\n",
    "                 sample_null_action=False,\n",
    "                 noise_abs_cost=False):\n",
    "        \"\"\"\n",
    "        :param dynamics: function(state, action) -> next_state (K x nx) taking in batch state (K x nx) and action (K x nu)\n",
    "        :param running_cost: function(state, action) -> cost (K) taking in batch state and action (same as dynamics)\n",
    "        :param nx: state dimension\n",
    "        :param noise_sigma: (nu x nu) control noise covariance (assume v_t ~ N(u_t, noise_sigma))\n",
    "        :param num_samples: K, number of trajectories to sample\n",
    "        :param horizon: T, length of each trajectory\n",
    "        :param device: pytorch device\n",
    "        :param terminal_state_cost: function(state) -> cost (K x 1) taking in batch state\n",
    "        :param lambda_: temperature, positive scalar where larger values will allow more exploration\n",
    "        :param noise_mu: (nu) control noise mean (used to bias control samples); defaults to zero mean\n",
    "        :param u_min: (nu) minimum values for each dimension of control to pass into dynamics\n",
    "        :param u_max: (nu) maximum values for each dimension of control to pass into dynamics\n",
    "        :param u_init: (nu) what to initialize new end of trajectory control to be; defeaults to zero\n",
    "        :param U_init: (T x nu) initial control sequence; defaults to noise\n",
    "        :param step_dependent_dynamics: whether the passed in dynamics needs horizon step passed in (as 3rd arg)\n",
    "        :param rollout_samples: M, number of state trajectories to rollout for each control trajectory\n",
    "            (should be 1 for deterministic dynamics and more for models that output a distribution)\n",
    "        :param rollout_var_cost: Cost attached to the variance of costs across trajectory rollouts\n",
    "        :param rollout_var_discount: Discount of variance cost over control horizon\n",
    "        :param sample_null_action: Whether to explicitly sample a null action (bad for starting in a local minima)\n",
    "        :param noise_abs_cost: Whether to use the absolute value of the action noise to avoid bias when all states have the same cost\n",
    "        \"\"\"\n",
    "        self.d = device\n",
    "        self.dtype = noise_sigma.dtype\n",
    "        self.K = num_samples  # N_SAMPLES\n",
    "        self.T = horizon  # TIMESTEPS\n",
    "\n",
    "        # dimensions of state and control\n",
    "        self.nx = nx\n",
    "        self.nu = 1 if len(noise_sigma.shape) == 0 else noise_sigma.shape[0]\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        if noise_mu is None:\n",
    "            noise_mu = torch.zeros(self.nu, dtype=self.dtype)\n",
    "\n",
    "        if u_init is None:\n",
    "            u_init = torch.zeros_like(noise_mu)\n",
    "\n",
    "        # handle 1D edge case\n",
    "        if self.nu == 1:\n",
    "            noise_mu = noise_mu.view(-1)\n",
    "            noise_sigma = noise_sigma.view(-1, 1)\n",
    "\n",
    "        # bounds\n",
    "        self.u_min = u_min\n",
    "        self.u_max = u_max\n",
    "        self.u_scale = u_scale\n",
    "        self.u_per_command = u_per_command\n",
    "        # make sure if any of them is specified, both are specified\n",
    "        if self.u_max is not None and self.u_min is None:\n",
    "            if not torch.is_tensor(self.u_max):\n",
    "                self.u_max = torch.tensor(self.u_max)\n",
    "            self.u_min = -self.u_max\n",
    "        if self.u_min is not None and self.u_max is None:\n",
    "            if not torch.is_tensor(self.u_min):\n",
    "                self.u_min = torch.tensor(self.u_min)\n",
    "            self.u_max = -self.u_min\n",
    "        if self.u_min is not None:\n",
    "            self.u_min = self.u_min.to(device=self.d)\n",
    "            self.u_max = self.u_max.to(device=self.d)\n",
    "\n",
    "        self.noise_mu = noise_mu.to(self.d)\n",
    "        self.noise_sigma = noise_sigma.to(self.d)\n",
    "        self.noise_sigma_inv = torch.inverse(self.noise_sigma)\n",
    "        self.noise_dist = MultivariateNormal(self.noise_mu, covariance_matrix=self.noise_sigma)\n",
    "        # T x nu control sequence\n",
    "        self.U = U_init\n",
    "        self.u_init = u_init.to(self.d)\n",
    "\n",
    "        if self.U is None:\n",
    "            self.U = self.noise_dist.sample((self.T,))\n",
    "\n",
    "        self.step_dependency = step_dependent_dynamics\n",
    "        self.F = dynamics\n",
    "        self.running_cost = running_cost\n",
    "        self.terminal_state_cost = terminal_state_cost\n",
    "        self.sample_null_action = sample_null_action\n",
    "        self.noise_abs_cost = noise_abs_cost\n",
    "        self.state = None\n",
    "\n",
    "        # handling dynamics models that output a distribution (take multiple trajectory samples)\n",
    "        self.M = rollout_samples\n",
    "        self.rollout_var_cost = rollout_var_cost\n",
    "        self.rollout_var_discount = rollout_var_discount\n",
    "\n",
    "        # sampled results from last command\n",
    "        self.cost_total = None\n",
    "        self.cost_total_non_zero = None\n",
    "        self.omega = None\n",
    "        self.states = None\n",
    "        self.actions = None\n",
    "\n",
    "    @handle_batch_input(n=2)\n",
    "    def _dynamics(self, state, u, t):\n",
    "        return self.F(state, u, t) if self.step_dependency else self.F(state, u)\n",
    "\n",
    "    @handle_batch_input(n=2)\n",
    "    def _running_cost(self, state, u):\n",
    "        return self.running_cost(state, u)\n",
    "\n",
    "    def command(self, state, shift_nominal_trajectory=True):\n",
    "        \"\"\"\n",
    "        :param state: (nx) or (K x nx) current state, or samples of states (for propagating a distribution of states)\n",
    "        :param shift_nominal_trajectory: Whether to roll the nominal trajectory forward one step. This should be True\n",
    "        if the command is to be executed. If the nominal trajectory is to be refined then it should be False.\n",
    "        :returns action: (nu) best action\n",
    "        \"\"\"\n",
    "        if shift_nominal_trajectory:\n",
    "            # shift command 1 time step\n",
    "            self.U = torch.roll(self.U, -1, dims=0)\n",
    "            self.U[-1] = self.u_init\n",
    "\n",
    "        return self._command(state)\n",
    "\n",
    "    def _command(self, state):\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state)\n",
    "        self.state = state.to(dtype=self.dtype, device=self.d)\n",
    "        cost_total = self._compute_total_cost_batch()\n",
    "        beta = torch.min(cost_total)\n",
    "        self.cost_total_non_zero = _ensure_non_zero(cost_total, beta, 1 / self.lambda_)\n",
    "        eta = torch.sum(self.cost_total_non_zero)\n",
    "        self.omega = (1. / eta) * self.cost_total_non_zero\n",
    "        perturbations = []\n",
    "        for t in range(self.T):\n",
    "            perturbations.append(torch.sum(self.omega.view(-1, 1) * self.noise[:, t], dim=0))\n",
    "        perturbations = torch.stack(perturbations)\n",
    "        self.U = self.U + perturbations\n",
    "        action = self.U[:self.u_per_command]\n",
    "        # reduce dimensionality if we only need the first command\n",
    "        if self.u_per_command == 1:\n",
    "            action = action[0]\n",
    "        return action\n",
    "\n",
    "    def change_horizon(self, horizon):\n",
    "        if horizon < self.U.shape[0]:\n",
    "            # truncate trajectory\n",
    "            self.U = self.U[:horizon]\n",
    "        elif horizon > self.U.shape[0]:\n",
    "            # extend with u_init\n",
    "            self.U = torch.cat((self.U, self.u_init.repeat(horizon - self.U.shape[0], 1)))\n",
    "        self.T = horizon\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Clear controller state after finishing a trial\n",
    "        \"\"\"\n",
    "        self.U = self.noise_dist.sample((self.T,))\n",
    "\n",
    "    def _compute_rollout_costs(self, perturbed_actions):\n",
    "        K, T, nu = perturbed_actions.shape\n",
    "        assert nu == self.nu\n",
    "\n",
    "        cost_total = torch.zeros(K, device=self.d, dtype=self.dtype)\n",
    "        cost_samples = cost_total.repeat(self.M, 1)\n",
    "        cost_var = torch.zeros_like(cost_total)\n",
    "\n",
    "        # allow propagation of a sample of states (ex. to carry a distribution), or to start with a single state\n",
    "        if self.state.shape == (K, self.nx):\n",
    "            state = self.state\n",
    "        else:\n",
    "            state = self.state.view(1, -1).repeat(K, 1)\n",
    "\n",
    "        # rollout action trajectory M times to estimate expected cost\n",
    "        state = state.repeat(self.M, 1, 1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        for t in range(T):\n",
    "            u = self.u_scale * perturbed_actions[:, t].repeat(self.M, 1, 1)\n",
    "            state = self._dynamics(state, u, t)\n",
    "            c = self._running_cost(state, u)\n",
    "            cost_samples = cost_samples + c\n",
    "            if self.M > 1:\n",
    "                cost_var += c.var(dim=0) * (self.rollout_var_discount ** t)\n",
    "\n",
    "            # Save total states/actions\n",
    "            states.append(state)\n",
    "            actions.append(u)\n",
    "\n",
    "        # Actions is K x T x nu\n",
    "        # States is K x T x nx\n",
    "        actions = torch.stack(actions, dim=-2)\n",
    "        states = torch.stack(states, dim=-2)\n",
    "\n",
    "        # action perturbation cost\n",
    "        if self.terminal_state_cost:\n",
    "            c = self.terminal_state_cost(states, actions)\n",
    "            cost_samples = cost_samples + c\n",
    "        cost_total = cost_total + cost_samples.mean(dim=0)\n",
    "        cost_total = cost_total + cost_var * self.rollout_var_cost\n",
    "        return cost_total, states, actions\n",
    "\n",
    "    def _compute_total_cost_batch(self):\n",
    "        # parallelize sampling across trajectories\n",
    "        # resample noise each time we take an action\n",
    "        noise = self.noise_dist.rsample((self.K, self.T))\n",
    "        # broadcast own control to noise over samples; now it's K x T x nu\n",
    "        perturbed_action = self.U + noise\n",
    "        if self.sample_null_action:\n",
    "            perturbed_action[self.K - 1] = 0\n",
    "        # naively bound control\n",
    "        self.perturbed_action = self._bound_action(perturbed_action)\n",
    "        # bounded noise after bounding (some got cut off, so we don't penalize that in action cost)\n",
    "        self.noise = self.perturbed_action - self.U\n",
    "        if self.noise_abs_cost:\n",
    "            action_cost = self.lambda_ * torch.abs(self.noise) @ self.noise_sigma_inv\n",
    "            # NOTE: The original paper does self.lambda_ * torch.abs(self.noise) @ self.noise_sigma_inv, but this biases\n",
    "            # the actions with low noise if all states have the same cost. With abs(noise) we prefer actions close to the\n",
    "            # nomial trajectory.\n",
    "        else:\n",
    "            action_cost = self.lambda_ * self.noise @ self.noise_sigma_inv  # Like original paper\n",
    "\n",
    "        rollout_cost, self.states, actions = self._compute_rollout_costs(self.perturbed_action)\n",
    "        self.actions = actions / self.u_scale\n",
    "\n",
    "        # action perturbation cost\n",
    "        perturbation_cost = torch.sum(self.U * action_cost, dim=(1, 2))\n",
    "        self.cost_total = rollout_cost + perturbation_cost\n",
    "        return self.cost_total\n",
    "\n",
    "    def _bound_action(self, action):\n",
    "        if self.u_max is not None:\n",
    "            for t in range(self.T):\n",
    "                u = action[:, self._slice_control(t)]\n",
    "                cu = torch.max(torch.min(u, self.u_max), self.u_min)\n",
    "                action[:, self._slice_control(t)] = cu\n",
    "        return action\n",
    "\n",
    "    def _slice_control(self, t):\n",
    "        return slice(t * self.nu, (t + 1) * self.nu)\n",
    "\n",
    "    def get_rollouts(self, state, num_rollouts=1):\n",
    "        \"\"\"\n",
    "            :param state: either (nx) vector or (num_rollouts x nx) for sampled initial states\n",
    "            :param num_rollouts: Number of rollouts with same action sequence - for generating samples with stochastic\n",
    "                                 dynamics\n",
    "            :returns states: num_rollouts x T x nx vector of trajectories\n",
    "\n",
    "        \"\"\"\n",
    "        state = state.view(-1, self.nx)\n",
    "        if state.size(0) == 1:\n",
    "            state = state.repeat(num_rollouts, 1)\n",
    "\n",
    "        T = self.U.shape[0]\n",
    "        states = torch.zeros((num_rollouts, T + 1, self.nx), dtype=self.U.dtype, device=self.U.device)\n",
    "        states[:, 0] = state\n",
    "        for t in range(T):\n",
    "            states[:, t + 1] = self._dynamics(states[:, t].view(num_rollouts, -1),\n",
    "                                              self.u_scale * self.U[t].view(num_rollouts, -1), t)\n",
    "        return states[:, 1:]\n",
    "\n",
    "\n",
    "def run_mppi(mppi, env, retrain_dynamics, retrain_after_iter=50, iter=1000, render=True):\n",
    "    dataset = torch.zeros((retrain_after_iter, mppi.nx + mppi.nu), dtype=mppi.U.dtype, device=mppi.d)\n",
    "    rewards = []\n",
    "    for i in range(iter):\n",
    "        state = env.state.copy()\n",
    "        command_start = time.perf_counter()\n",
    "        action = mppi.command(state)\n",
    "        elapsed = time.perf_counter() - command_start\n",
    "        s, r, _, _ = env.step(action.detach().numpy())\n",
    "        rewards.append(r)\n",
    "        # logger.debug(\"action taken: %.4f reward received: %.4f time taken: %.5fs\", action, r, elapsed)\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        di = i % retrain_after_iter\n",
    "        if di == 0 and i > 0:\n",
    "            retrain_dynamics(dataset)\n",
    "            # don't have to clear dataset since it'll be overridden, but useful for debugging\n",
    "            dataset.zero_()\n",
    "        dataset[di, :mppi.nx] = torch.tensor(state, dtype=mppi.U.dtype)\n",
    "        dataset[di, mppi.nx:] = action\n",
    "    return rewards, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward:  -8.322679148187145\n",
      "Average reward:  -7.02474299975023\n",
      "Average reward:  -7.186674658710171\n",
      "Average reward:  -7.822397375158583\n",
      "Average reward:  -6.184052794451324\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NN_models/NN_51000.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m timesteps \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m, \u001b[39m52000\u001b[39m, \u001b[39m10000\u001b[39m)):\n\u001b[1;32m     35\u001b[0m     model \u001b[39m=\u001b[39m Net()\n\u001b[0;32m---> 36\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(FOLDER\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/NN_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(timesteps)))\n\u001b[1;32m     38\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdynamics\u001b[39m(state, perturbed_action):\n\u001b[1;32m     39\u001b[0m         \u001b[39m# NN prediction\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[39m# join state and action\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         state_action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((state, perturbed_action), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/zhiyu39/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/zhiyu39/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/zhiyu39/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NN_models/NN_51000.pt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "result = pd.DataFrame(columns=[\"Timesteps_trained\", \"rewards_mean\", \"rewards_std\", \"rewards_max\", \"rewards_min\", \"rewards_median\", \"rewards_25\", \"rewards_75\"])\n",
    "\n",
    "FOLDER = 'NN_models'\n",
    "ENV_NAME = \"Pendulum-v1\"\n",
    "TIMESTEPS = 15  # T\n",
    "N_SAMPLES = 100  # K\n",
    "ACTION_LOW = -2.0\n",
    "ACTION_HIGH = 2.0\n",
    "\n",
    "d = \"cuda\"\n",
    "dtype = torch.double\n",
    "\n",
    "noise_sigma = torch.tensor(10, device=d, dtype=dtype)\n",
    "# noise_sigma = torch.tensor([[10, 0], [0, 10]], device=d, dtype=dtype)\n",
    "lambda_ = 1.\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x + math.pi) % (2 * math.pi)) - math.pi)\n",
    "\n",
    "\n",
    "def running_cost(state, action):\n",
    "    theta = state[:, 0]\n",
    "    theta_dt = state[:, 1]\n",
    "    action = action[:, 0]\n",
    "    cost = angle_normalize(theta) ** 2 + 0.1 * theta_dt ** 2\n",
    "    return cost\n",
    "\n",
    "def train(new_data):\n",
    "    pass\n",
    "\n",
    "for timesteps in list(range(1000, 52000, 10000)):\n",
    "\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(FOLDER+'/NN_{}.pt'.format(timesteps)))\n",
    "\n",
    "    def dynamics(state, perturbed_action):\n",
    "        # NN prediction\n",
    "        # join state and action\n",
    "        state_action = torch.cat((state, perturbed_action), dim=1)\n",
    "        \n",
    "        # make state_action type dtype=torch.float32\n",
    "        state_action = state_action.type(dtype=torch.float32)\n",
    "        prediction = model(state_action)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    downward_start = False\n",
    "    env = gym.make(ENV_NAME).env  # bypass the default TimeLimit wrapper\n",
    "    env.reset()\n",
    "    if downward_start:\n",
    "        env.state = [np.pi, 1]\n",
    "    nx = 2\n",
    "    mppi_gym = MPPI(dynamics, running_cost, nx, noise_sigma, num_samples=N_SAMPLES, horizon=TIMESTEPS,\n",
    "                            lambda_=lambda_)\n",
    "\n",
    "    iteration = 10000\n",
    "    rewards, dataset = run_mppi(mppi=mppi_gym, env=env, retrain_dynamics=train, retrain_after_iter=50, iter=iteration, render=False)\n",
    "\n",
    "    # calculate statistics\n",
    "    rewards_mean = np.mean(rewards)\n",
    "    rewards_std = np.std(rewards)\n",
    "    rewards_max = np.max(rewards)\n",
    "    rewards_min = np.min(rewards)\n",
    "    rewards_median = np.median(rewards)\n",
    "    rewards_25 = np.percentile(rewards, 25)\n",
    "    rewards_75 = np.percentile(rewards, 75)\n",
    "    result.loc[len(result)] = [timesteps, rewards_mean, rewards_std, rewards_max, rewards_min, rewards_median, rewards_25, rewards_75]\n",
    "\n",
    "    print(\"Average reward: \", rewards_mean)\n",
    "\n",
    "\n",
    "result.to_csv(FOLDER+\"_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhiyu39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "210d99c3fa445973a1a9dbd666f41525256243834c70425bd71fc0f1f877f877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
