{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def random_shooting_mpc(start_state, model, reward_fn, horizon, n_samples, gamma):\n",
    "    \"\"\"\n",
    "    Random shooting model predictive control for the Pendulum-v1 environment in OpenAI Gym.\n",
    "    \n",
    "    Args:\n",
    "    - start_state (numpy array): the initial state of the environment\n",
    "    - model (function): a function that takes a state and an action, and returns the next state\n",
    "    - reward_fn (function): a function that takes a state and returns a reward\n",
    "    - horizon (int): the number of timesteps in the MPC horizon\n",
    "    - n_samples (int): the number of control sequences to sample\n",
    "    - gamma (float): the discount factor\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_control (numpy array): the optimal control sequence for the MPC horizon\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the control sequences randomly\n",
    "    control_sequences = np.random.uniform(low=-2.0, high=2.0, size=(n_samples, horizon))\n",
    "    \n",
    "    # Evaluate the control sequences\n",
    "    rewards = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        state = start_state\n",
    "        for t in range(horizon):\n",
    "            action = control_sequences[i, t]\n",
    "            next_state = model(state, action)\n",
    "            rewards[i] += reward_fn(next_state, action) * gamma**t\n",
    "            state = next_state\n",
    "    \n",
    "    # Find the optimal control sequence\n",
    "    optimal_index = np.argmax(rewards)\n",
    "    optimal_control = control_sequences[optimal_index]\n",
    "    \n",
    "    return optimal_control\n",
    "\n",
    "\n",
    "def pendulum_model(state, action):\n",
    "    temp_env = gym.make('Pendulum-v1')\n",
    "    temp_env.reset()\n",
    "    temp_env.state = state\n",
    "    next_state, _, _, _ = temp_env.step([action])\n",
    "    temp_env.close()\n",
    "    return next_state\n",
    "\n",
    "def pendulum_reward(state, action):\n",
    "    temp_env = gym.make('Pendulum-v1')\n",
    "    temp_env.reset()\n",
    "    temp_env.state = state\n",
    "    cos_theta, sin_theta, theta_dot = temp_env.state\n",
    "    temp_env.close()\n",
    "    return -(theta_dot**2 + 0.1*cos_theta**2 + 0.001*(action**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.98\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "for horizon in [10, 15, 20]:\n",
    "    for n_sample in [1000, 2000, 3000]:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
