{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "def random_shooting_mpc(start_state, model, reward_fn, horizon, n_samples, gamma):\n",
    "    \"\"\"\n",
    "    Random shooting model predictive control for the Pendulum-v1 environment in OpenAI Gym.\n",
    "    \n",
    "    Args:\n",
    "    - start_state (numpy array): the initial state of the environment\n",
    "    - model (function): a function that takes a state and an action, and returns the next state\n",
    "    - reward_fn (function): a function that takes a state and returns a reward\n",
    "    - horizon (int): the number of timesteps in the MPC horizon\n",
    "    - n_samples (int): the number of control sequences to sample\n",
    "    - gamma (float): the discount factor\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_control (numpy array): the optimal control sequence for the MPC horizon\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the control sequences randomly\n",
    "    control_sequences = np.random.uniform(low=-2.0, high=2.0, size=(n_samples, horizon))\n",
    "\n",
    "    # tensorize control_sequences\n",
    "    control_sequences = torch.tensor(control_sequences, dtype=torch.float32)\n",
    "    \n",
    "    # Evaluate the control sequences\n",
    "    rewards = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        state = start_state\n",
    "        for t in range(horizon):\n",
    "            action = control_sequences[i, t]\n",
    "            next_state = model(state, action)\n",
    "            rewards[i] += reward_fn(next_state, action) * gamma**t\n",
    "            state = next_state\n",
    "    \n",
    "    # Find the optimal control sequence\n",
    "    optimal_index = np.argmax(rewards)\n",
    "    optimal_control = control_sequences[optimal_index]\n",
    "    \n",
    "    return optimal_control\n",
    "\n",
    "\n",
    "def pendulum_model(state, perturbed_action):\n",
    "    # true dynamics from gym\n",
    "    # state: [cos(theta), sin(theta), theta_dot]\n",
    "    # th is angle from vertical\n",
    "    th = math.atan2(state[1], state[0])\n",
    "    thdot = state[2]\n",
    "\n",
    "    g = 10\n",
    "    m = 1\n",
    "    l = 1\n",
    "    dt = 0.05\n",
    "\n",
    "    u = perturbed_action\n",
    "    u = torch.clamp(u, -2, 2)\n",
    "\n",
    "    newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
    "    newth = th + newthdot * dt\n",
    "    newthdot = torch.clamp(newthdot, -8, 8)\n",
    "\n",
    "    state = torch.cat((newth, newthdot), dim=1)\n",
    "    return state\n",
    "\n",
    "def pendulum_reward(state, action):\n",
    "    cos_theta, sin_theta, theta_dot = state[0], state[1], state[2]\n",
    "    return -(theta_dot**2 + 0.1*cos_theta**2 + 0.001*(action**2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9620051   0.27303156 -0.505954  ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     action \u001b[39m=\u001b[39m random_shooting_mpc(state, pendulum_model, pendulum_reward, horizon, n_samples, gamma)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep([action])\n\u001b[1;32m     14\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[26], line 33\u001b[0m, in \u001b[0;36mrandom_shooting_mpc\u001b[0;34m(start_state, model, reward_fn, horizon, n_samples, gamma)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(horizon):\n\u001b[1;32m     32\u001b[0m     action \u001b[39m=\u001b[39m control_sequences[i, t]\n\u001b[0;32m---> 33\u001b[0m     next_state \u001b[39m=\u001b[39m model(state, action)\n\u001b[1;32m     34\u001b[0m     rewards[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward_fn(next_state, action) \u001b[39m*\u001b[39m gamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mt\n\u001b[1;32m     35\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[26], line 63\u001b[0m, in \u001b[0;36mpendulum_model\u001b[0;34m(state, perturbed_action)\u001b[0m\n\u001b[1;32m     60\u001b[0m newth \u001b[39m=\u001b[39m th \u001b[39m+\u001b[39m newthdot \u001b[39m*\u001b[39m dt\n\u001b[1;32m     61\u001b[0m newthdot \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(newthdot, \u001b[39m-\u001b[39m\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((newth, newthdot), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m state\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "horizon = 10\n",
    "n_samples = 1000\n",
    "gamma = 0.98\n",
    "\n",
    "state = env.reset()\n",
    "rewards = []\n",
    "print(state)\n",
    "for i in range(10000):\n",
    "    action = random_shooting_mpc(state, pendulum_model, pendulum_reward, horizon, n_samples, gamma)[0]\n",
    "    next_state, reward, done, info = env.step([action])\n",
    "    state = next_state\n",
    "    rewards.append(reward)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"at step \" + str(i))\n",
    "\n",
    "# calculate statistics\n",
    "rewards_mean = np.mean(rewards)\n",
    "rewards_std = np.std(rewards)\n",
    "rewards_max = np.max(rewards)\n",
    "rewards_min = np.min(rewards)\n",
    "rewards_median = np.median(rewards)\n",
    "rewards_25 = np.percentile(rewards, 25)\n",
    "rewards_75 = np.percentile(rewards, 75)\n",
    "\n",
    "print(\"mean: \" + str(rewards_mean))\n",
    "print(\"std: \" + str(rewards_std))\n",
    "print(\"max: \" + str(rewards_max))\n",
    "print(\"min: \" + str(rewards_min))\n",
    "print(\"median: \" + str(rewards_median))\n",
    "print(\"25: \" + str(rewards_25))\n",
    "print(\"75: \" + str(rewards_75))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhiyu39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "210d99c3fa445973a1a9dbd666f41525256243834c70425bd71fc0f1f877f877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
